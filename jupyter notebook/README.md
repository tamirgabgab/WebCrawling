
## URL's
Given in the code as list or url's, it is optional to read from .txt file (with space or '\n' as seperators) at the same directory.

## Constraints
We assume a maximum url's visits per main url (max_urls variable) and limited the number of entrances per link (max_link variable)

## Files

### crawler_pyrhon.ipynb
Contain all the scrifts in **python** version of web crawling.

### figure1
the result of crawling among all url's



## Dependencies
Google colab <br>

BeautifulSoup <br>
collections <br>
tldextract <br>
matplotlib <br>
threading <br>
requests <br>
networkx <br>
random <br>
pandas <br>
urllib <br>
pprint <br>
mpld3 <br>
numpy <br>
time <br>
re <br>

all the module (except mpld3, and tldextract) are availalbe in Google colab, use **!pip install mpld3** and **!pip install mpld3** commands for install mpld3 module (whice is for interactive results in html file)


## Results Example
shown in figure1.html (download and open the file in **Chrome**)
