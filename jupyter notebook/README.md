
## URL's
Given in the code as list or url's, it is optional to read from .txt file (with space or '\n' as seperators) at the same directory.

## Constraints
We assume a maximum url's visits per main url (max_urls variable) and limited the number of entrances per link (max_link variable)

## Files

### crawler_pyrhon.ipynb
Contain all the scrifts in **python** version of web crawling.

### figure1
the result of crawling among all url's

## Example of result of 2 url's

![image](https://user-images.githubusercontent.com/80973047/166524001-75b970b4-8d65-49cf-8c12-2ec5c447a115.png)
<br>
![image](https://user-images.githubusercontent.com/80973047/166523745-a9d0632c-f3ab-4c61-a73c-d41aa9bb8968.png)

## Dependencies
Google colab <br>

BeautifulSoup <br>
collections <br>
tldextract <br>
matplotlib <br>
threading <br>
requests <br>
networkx <br>
random <br>
pandas <br>
urllib <br>
pprint <br>
mpld3 <br>
numpy <br>
time <br>
re <br>

all the module (except mpld3, and tldextract) are availalbe in Google colab, use **!pip install mpld3** and **!pip install mpld3** commands for install mpld3 module (whice is for interactive results in html file)


## Results Example
shown in figure1.html (download and open the file in **Chrome**)
